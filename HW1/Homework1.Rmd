---
title: "Homework 1"
author: "Ian Frankenburg"
date: "4/7/2020"
header-includes:
   - \usepackage{bm}
   - \usepackage{algorithmic}
   - \usepackage[]{algorithm2e}
   - \usepackage{tikz,lipsum,lmodern}
   - \usepackage[most]{tcolorbox}
   - \usepackage{setspace}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
library(kableExtra)
library(latex2exp)
library(mvtnorm)
library(invgamma)
library(lars)
library(matrixStats)
library(statmod)
library(glmnet)
library(tidyverse)
library(dplyr)
library(tidyr)
library(Rcpp)
```

# Bayesian Adaptive Lasso
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part a.}]
Consider $p = 1$. Simulate 5,000 Monte Carlo samples from the conditional prior $\pmb\beta | \tau^2 = 1$ and obtain a plot of the density using the \textsf{R} function density.
\end{tcolorbox}
```{r,fig.height = 3.5, fig.width = 3.5, fig.align = "center"}
n <- 5000
plot(density(rnorm(n,0,1)), main=TeX(paste("$\\beta$", "marginal")))
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part b.}]
Consider $p = 1$. Simulate 5,000 Monte Carlo samples from the marginal prior $\pmb\beta$, considering $\lambda^2 = 2$, so that $\mathbb E(\tau^2|\lambda) = 1$. Obtain a plot of the density as in \textbf{a.}
\end{tcolorbox}

```{r, fig.height = 3.5, fig.width = 3.5, fig.align = "center"}
lambda <- sqrt(2)
tau.sq <- rgamma(n,shape=1,rate = lambda^2/2)
beta.marginal <- rnorm(n,0,sqrt(tau.sq))
plot(density(beta.marginal), main=TeX(paste("$\\lambda^2 = 2$")), xlim=c(-5,5))
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part c.}]
Consider $p = 1$. Add a hyper prior on $\gamma = 1/\gamma \sim Gamma(a,rate = b)$. Assess how the marginal prior of $\pmb\beta$ changes for $a = 1$ and values of $b \geq 1$.
\end{tcolorbox}

```{r}
set.seed(1)
par(mfrow=c(2,2)) 
rates <- c(1,3,5,10)
for(b in rates){
  lambda <- 1/rgamma(n,1,b)
  tau.sq <- rgamma(n,shape=1,rate = lambda^2/2)
  beta.marginal <- rnorm(n,0,sqrt(tau.sq))
  plot(density(beta.marginal), main=paste("rate b = ",b),xlim=c(-5,5))
}
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part d.}]
Considering the hyper prior in \textbf{c.}, describe a Markov Chain Monte Carlo algorithm to sample from the posterior distribution of $\pmb\beta$ and $\sigma^2$.
\end{tcolorbox}

I will implement a joint Gibbs and Metropolis sampler. The model is
$$
\begin{aligned}
\pmb y|\pmb\beta,\sigma^2 &\sim N(\pmb{X\beta},\sigma^2\pmb{I})\\
\beta_j|\tau^2_j &\sim N(0,\tau^2_j)\\
\tau^2_j &\sim \text{Inverse-Gamma}(1,\frac{\lambda^2}{2})\\
\lambda &\sim \text{Inverse-Gamma}(a,1/b)\\
\sigma^2 &\sim \text{Inverse-Gamma}(0.1,0.1).
\end{aligned}
$$
I need the full conditionals
$$
\begin{aligned}
\{\beta_1,\ldots,\beta_p| \pmb y,\sigma^2,\tau_1^2,\ldots,\tau_p^2, \lambda\},\\
\{\sigma^2| \pmb y,\beta_1,\ldots,\beta_p,\tau_1^2,\ldots,\tau_p^2, \lambda\},\\
\{\tau_1^2,\ldots,\tau^2_p| \pmb y,\beta_1,\ldots,\beta_p,\sigma^2, \lambda\},\\
\{\lambda| \pmb y,\beta_1,\ldots,\beta_p,\sigma^2,\tau_1^2,\ldots,\tau_p^2\}
\end{aligned}
$$
which are all proportional to
$$
p(\pmb y|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)
\times p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)
\times p(\tau^2_1,\ldots,\tau^2_p|\lambda)p(\lambda)p(\sigma^2)
$$

so I'll start with the posterior
$$
\begin{aligned}
p(\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda|\pmb y)&\propto p(\pmb y|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)\\
&\qquad \times p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)\\
&\qquad \times p(\tau^2_1,\ldots,\tau^2_p|\lambda)p(\lambda)p(\sigma^2).
\end{aligned}
$$
As a function of just $\sigma^2$, this is proportional to
$$
\begin{aligned}
  &p(\pmb{y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)p(\sigma^2)\\
  =&\quad N(\pmb{X\beta},\sigma^2\pmb I)IG(0.1,0.1).
\end{aligned}
$$
Time to show this is inverse-gamma distributed.
$$
\begin{aligned}
&\quad N(\pmb y;\pmb{X\beta},\sigma^2\pmb I)IG(\sigma^2;q,r)\\
\propto \ &(\sigma^2)^{-n/2}\exp\big\{-\frac{1}{2\sigma^2}(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})\big\}(\sigma^2)^{q-1}\exp\big\{-\frac{r}{\sigma^2}\big\}\\
= \ &(\sigma^2)^{-(n/2+q)-1}\exp\big\{-\frac{1}{\sigma^2}(r+\frac{1}{2}(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})\big\}\\
= \ & IG(n/2+q, r+(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})/2)|_{q=0.1,r=0.1}
\end{aligned}
$$
As a function of $\pmb\beta$, the conditional is proportional to
$$
\begin{aligned}
  &p(\pmb y|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)
  p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)\\
  =&\quad N(\pmb{X\beta},\sigma^2\pmb I)\cdot\prod_{i=1}^pN(0,\tau_i^2)\\
  =& \quad N(\pmb{X\beta},\pmb\Sigma)\cdot N(0,\pmb\Omega),\text{ where }\Omega=\text{diag}(\tau_1^2,\ldots,\tau_p^2)\\
  =& \quad N(\pmb m,\pmb M)
\end{aligned}
$$
because the posterior is determined by the quadratic form
$$
\begin{aligned}
(\pmb y-\pmb{X\beta})^\top\Sigma^{-1}(\pmb y-\pmb{X\beta})+\pmb\beta^\top\Omega^{-1}\pmb\beta=(\pmb\beta-\pmb m)^\top\pmb M^{-1}(\pmb\beta-\pmb m).
\end{aligned}
$$
Completing the square gives $\pmb m=\pmb M\pmb X^\top\pmb\Sigma^{-1}\pmb y$ and $\pmb M=(\pmb X^\top\pmb\Sigma^{-1}\pmb X+\Omega^{-1})^{-1}$.

As a function of $\tau_1^2,\ldots,\tau^2_p$, the target is proportional to
$$
\begin{aligned}
  &p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)p(\tau_1^2,\ldots,\tau_p^2|\lambda)\\
  =&\quad\prod_{i=1}^pN(\beta_i;0,\tau_i^2)\cdot\prod_{i=1}^p IG(\tau^2_i;1,\frac{\lambda^2}{2})
\end{aligned}
$$
Finally, as a function of $\lambda$, the target distribution is proportional to
$$
\begin{aligned}
  &p(\tau_1^2,\ldots,\tau_p^2|\lambda)p(\lambda)\\
  =&\quad\prod_{i=1}^pIG(\tau_i^2;1,\frac{\lambda^2}{2})\cdot IG(\lambda; a,b)\\
\end{aligned}
$$
Now I can build an algorithm to iteratively update through these target distributions.
I take the starting value of $\pmb\beta^{(0)}$ to be the least-squares solution $\pmb{\hat\beta}$ along with $\sigma^{2(0)}=\hat\sigma^{2}$, the MLE for $\sigma^2$.

\begin{algorithm}
\DontPrintSemicolon
\setstretch{1.5}
  \KwResult{Samples from joint posterior $p(\pmb\beta,\sigma^2|\pmb y)$ }
  \For{s in \# samples}{
    $\text{note: extra term due in } logr \text{ due to Jacobian of transformation}$
    $\lambda^*\gets \exp(\log(\lambda^{(s)})+\varepsilon),\quad \varepsilon\sim N(0,\delta^2)$\;
    $logr\gets\log\pi_\lambda(\lambda^*)-\log\pi_\lambda\lambda^{(s)}+\log\lambda^*-\log\lambda^{(s)}$\;
    \eIf{$(\log unif(0,1)<logr)$}{
        $\lambda^{(s+1)}\gets\lambda^*$
    }{
        $\lambda^{(s+1)}\gets\lambda^{(s)}$
    }
    \For{j in 1:p}{
    $\text{note: extra term due in } logr \text{ due to Jacobian of transformation}$
    $\tau^{2*}_j \gets \exp(\log(\tau^{2(s)}_j)+\varepsilon),\quad \varepsilon\sim N(0,\delta^2)$
    $logr \gets
      \log\pi_{\tau^2_j}(\tau_j^{2*}) -
      \log\pi_{\tau^2_j}(\tau_j^{2(s)}) +
      \log(\tau^{2*}_j)-\log(\tau^{2(s)}_j)$\;
    \eIf{$(\log unif(0,1)<logr)$}{
        $\tau^{2(s+1)}_j\gets\tau^{2*}_j$
    }{
        $\tau^{2(s+1)}_j\gets\tau^{2(s)}_j$
    }
  }
    $\sigma^{2(s+1)}\sim IG(n/2+a, 2b+(\pmb{y}-\pmb{X\beta}^{(s)})^\top(\pmb{y}-\pmb{X\beta}^{(s)})/2)$\;
    $\pmb\beta^{(s+1)}\sim N(\pmb m,\pmb M),\text{where}$\;
    $\pmb M=(\pmb X^\top\pmb\Sigma^{-1}\pmb X+\pmb\Omega^{-1})^{-1} \text{ and }
    \pmb m=\pmb M(\pmb X^\top\pmb\Sigma^{-1}\pmb y)$\;
    $\pmb\Sigma = \sigma^{2(s+1)}\text{ and }
    \pmb\Omega=\text{diag}(\tau_1^{2(s+1)},\ldots,\tau_p^{2(s+1)})$
}
\end{algorithm}

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part f.}]
Implement such algorithm in \textsf{R} and compare your results with estimates obtained using \textbf{glmnet()}. In particular, you should test your results on the diabetes data available from lars, (use the matrix of predictors x).
\end{tcolorbox}

```{r, eval=T,warning=F}
## Data processing
sourceCpp("helperFunctions.cpp")
set.seed(1)
data("diabetes")
X <- cbind(rep(1,length(diabetes$x)),cbind(diabetes$x)); y <- diabetes$y; 
n <- nrow(X); p <- ncol(X); samples <- 10000

## Initialize starting values
lambda <- 1
tau2 <- rep(1000,p)
beta <- solve(t(X)%*%X)%*%t(X)%*%y
sigma2 <- t(y-X%*%beta)%*%(y-X%*%beta)/n
sigma2.chain <- lambda.chain <- rep(0,samples)
beta.chain <- tau2.chain <- matrix(0,nrow=p,ncol=samples)
s <- 1
## MCMC
for(s in 2:samples){
  ####
  ### FIX X'X and known computations a priori
  ####
  lambda <- lambdaDraw(current=lambda,tau2=tau2,a=1,b=1)
  tau2 <- tau2Draw(current=tau2,beta=beta,lambda=lambda)
  sigma2 <- sigma2Draw(beta, y, X)
  mM <- betaMeanCov(y,X,sigma2,tau2)
  #beta <- t(rmvnorm(n=1,mean=mM$mean,sigma=mM$cov))
  beta <- rmvnorm_cpp(1,mu=mM$mean, Sigma=mM$cov)
  lambda.chain[s] <- lambda
  sigma2.chain[s] <- sigma2
  beta.chain[,s] <- beta
  tau2.chain[,s] <- tau2
}
# Examine markov chains
# plot(beta.chain[1,floor(samples/4):samples],type="l")

# Plot table of coefficients from Glmnet and Bayesian Lasso
comparison <- data.frame(
  "Bayesian Lasso" = rowMeans(beta.chain[,floor(samples/4):samples]),
  "Glmnet" = matrix(coef(glmnet(y=y,x=X),alpha=1,s=1)[-2])
)
kable(comparison, "latex", booktabs = T)
```

- I initially notice the difference in parameterization between glment's lasso and my Bayesian lasso. I'm viewing the coefficients for $\lambda=1$ in glmnet and a value of $b=1$ for Bayesian lasso. As I'll show later, in this implementation of Bayesian lasso, shrinkage is very sensitive to the hyperparameter $b$ of $\lambda$.

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part g.}]
Free $\lambda$ and carry out a sensitivity analysis assessing the behavior of the posterior distribution of $\pmb\beta$ and $\sigma^2$, as hyper parameters a and b are changed. Explain clearly the rationale you use to assess sensitivity and provide recommendations for the analysis of the diabetes data.
\end{tcolorbox}
```{r, eval=T,warning=F}
# Sequence of lambdas for comparison with glmnet
lambdas <- seq(from=-12, to = -1, length.out = 12)

# Keep track of posterior mean of beta for each fixed lambda
post.means <- matrix(NA,nrow=p,ncol=length(lambdas))

for(i in 1:length(lambdas)){
  for(s in 2:samples){
    lambda <- exp(lambdas[i])
    tau2 <- tau2Draw(current=tau2,beta=beta,lambda=lambda)
    sigma2 <- sigma2Draw(beta, y, X)
    mM <- betaMeanCov(y,X,sigma2,tau2)
    beta <- t(rmvnorm(n=1,mean=mM$mean,sigma=mM$cov))
    sigma2.chain[s] <- sigma2
    beta.chain[,s] <- beta
    tau2.chain[,s] <- tau2
  }
  post.means[,i] <- matrix(rowMeans(beta.chain[,floor(samples/4):samples]),nrow = p)
}
```

```{r, echo=F, out.width = '50%'}
fit <- glmnet(X, y)
glmnetCoef <- coef(fit,s=1, alpha=1)
plot(fit, xvar="lambda")
betaChain <- colMeans(beta.chain[,1:samples])
plot(lambdas,post.means[2,],ylab="Coefficients",xlab="log lambdas",ylim=c(min(post.means),max(post.means)),type="l")
lines(lambdas,post.means[2,])
for(j in 3:p){
  points(lambdas,post.means[j,],ylab="Coefficients",xlab="log lambdas", col=j,type="l")
  lines(lambdas,post.means[j,], col=j)
}
```

- Glmnet is on the left and Bayesian lasso on the right. These regularization paths look very similar.


\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part g.}]
Free $\lambda$ and carry out a sensitivity analysis assessing the behavior of the posterior distribution of $\pmb\beta$ and $\sigma^2$, as hyper parameters a and b are changed. Explain clearly the rationale you use to assess sensitivity and provide recommendations for the analysis of the diabetes data.
\end{tcolorbox}

```{r, eval=T,warning=F}
## Sequence of b's that define a path of hyperparameters for lambda
bs <- c(seq(from=1e-5,to=20,length.out = 30))
betasB <- matrix(0,nrow=p,ncol=length(bs))

## Keep track of posterior means for each b value whith lambda free
post.means <- matrix(NA,nrow=p,ncol=length(bs))

for(j in 1:length(bs)){
  for(s in 2:samples){
    lambda <- lambdaDraw(current=lambda,tau2=tau2,a=1,b=1/bs[j])
    tau2 <- tau2Draw(current=tau2,beta=beta, lambda=lambda)
    sigma2 <- sigma2Draw(beta, y, X)
    mM <- betaMeanCov(y,X,sigma2,tau2)
    beta <- t(rmvnorm(n=1,mean=mM$mean,sigma=mM$cov))
    beta.chain[,s] <- beta
  }
betasB[,j] <- rowMeans(beta.chain[,floor(samples/4):samples])
}
```

```{r, echo=F, out.width = '50%'}
plot(bs,betasB[2,],ylim = c(min(betasB),max(betasB)),type="l", main="Effect of 1/b", ylab="Coefficients",xlab="1/b's")
for(i in 3:p){
  lines(smooth.spline(bs,betasB[i,], spar=0.5), col=i)
  #plot(smoothingSpline, type="l") 
  #lines(spline(bs,betasB[i,], method = "natural"), col=i)
}
```

- Though I didn't include the plots, the shrinkage of coefficients seemed very robust to changes in $a$ for fixed $b$, so I chose to fix $a=1$ and focus on varying $b$. From the plot, I notice as $1/b$ approaches zero, the coefficients approach the least-squares estimates. As $1\beta$ increases, there's an increasing amount of shrinkage towards zero. This matches behavior of the regularization paths in part g as $lambda$ is fixed and increasing.

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part e.}]
\texttt{C++} Helper functions.
\end{tcolorbox}
```{Rcpp, eval=F}
#include <cmath>
#include <math.h>
#include <random>
#include <RcppArmadillo.h>
using namespace Rcpp;
using namespace std;

// [[Rcpp::depends(RcppArmadillo)]]

double loglambdaTarget(double lambda, vector<double> tau2, double a, double b) {
  return((-a-1)*log(lambda)-(pow(lambda,2)/2*accumulate(tau2.begin(),tau2.end(),0))-1/(b*lambda));
}

double logtau2jTarget(double tau2j, double lambda, double betaj){
  return(-log(sqrt(tau2j)) - 1.0/2.0*(1.0/tau2j*pow(betaj,2) + pow(lambda,2)*tau2j));
}

// [[Rcpp::export]]
double lambdaDraw(double current, vector<double> tau2, double a, double b){
  std::random_device rd;
  std::mt19937 mt(rd());
  std::uniform_real_distribution<double> dist(0, 1.0);
  std::normal_distribution<double> norm(0, current);
  double proposed = exp(log(current)+norm(mt));
  double logr = loglambdaTarget(proposed,tau2,1,b)-
    loglambdaTarget(current,tau2,1,b)+log(proposed)-
    log(current);
  if(log(dist(mt))<logr){
    current = proposed;
  }
  return current;
}

// [[Rcpp::export]]
vector<double> tau2Draw(vector<double> current, vector<double> beta, double lambda){
  std::random_device rd;
  std::mt19937 mt(rd());
  std::uniform_real_distribution<double> dist(0, 1.0);
  std::normal_distribution<double> norm(0, 1);
  for(int j=0;j < current.size(); j++){
    double tau2j_proposed = exp(log(current[j])+norm(mt));
    double logr =
      logtau2jTarget(tau2j_proposed, lambda, beta[j]) -
      logtau2jTarget(current[j], lambda, beta[j]) +
      log(tau2j_proposed)-log(current[j]);
    if(log(dist(mt)) < logr){
      current[j] = tau2j_proposed;
    }
  }
  return current;
}

// [[Rcpp::export]]
double sigma2Draw(const arma::vec & beta, const arma::vec & y, const arma::mat & X){
  std::random_device rd;
  std::mt19937 mt(rd());
  int n = X.n_rows;
  arma::colvec coef = arma::solve(X, y);
  arma::colvec resid = y - X*coef;
  double rss = arma::as_scalar(arma::trans(resid)*resid);
  std::gamma_distribution<double> gamma(n/2.0+0.1,1.0/(rss/2.0+0.1));
  return 1.0/gamma(mt);
}

// [[Rcpp::export]]
List betaMeanCov(const arma::vec & y, const arma::mat & X, double sigma2, const arma::vec & tau2){
  std::random_device rd;
  std::mt19937 mt(rd());
  int p = X.n_cols;
  arma::mat I = arma::eye(p,p);
  arma::mat tau2_inv = arma::inv(arma::diagmat(tau2));
  arma::mat M = arma::inv(X.t()*X/sigma2+tau2_inv);
  arma::colvec m = M*X.t()*y/sigma2;
  return List::create(Named("cov") = M, Named("mean")= m);
}


// List mcmc(double lambda, double sigma2, const arma::vec & tau2, const arma::vec & beta2){
//   
//   return List::create(Named("beta.chain") = M, Named("sigma2.chain")= m,
//                       Named("tau2.chain") = M, Named("lambda.chain") = M);
// }
```








