---
title: "Homework 1"
author: "Ian Frankenburg"
date: "4/7/2020"
header-includes:
   - \usepackage{bm}
   - \usepackage{algorithmic}
   - \usepackage[]{algorithm2e}
   - \usepackage{tikz,lipsum,lmodern}
   - \usepackage[most]{tcolorbox}
   - \usepackage{setspace}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
library(latex2exp)
library(mvtnorm)
library(invgamma)
library(lars)
library(matrixStats)
library(statmod)
library(glmnet)
```

# Bayesian Adaptive Lasso
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part a.}]
Consider $p = 1$. Simulate 5,000 Monte Carlo samples from the conditional prior $\pmb\beta | \tau^2 = 1$ and obtain a plot of the density using the \textsf{R} function density.
\end{tcolorbox}
```{r,fig.height = 3.5, fig.width = 3.5, fig.align = "center"}
n <- 5000
plot(density(rnorm(n,0,1)), main=TeX(paste("$\\beta$", "marginal")))
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part b.}]
Consider $p = 1$. Simulate 5,000 Monte Carlo samples from the marginal prior $\pmb\beta$, considering $\lambda^2 = 2$, so that $\mathbb E(\tau^2|\lambda) = 1$. Obtain a plot of the density as in \textbf{a.}
\end{tcolorbox}

```{r, fig.height = 3.5, fig.width = 3.5, fig.align = "center"}
lambda <- sqrt(2)
tau.sq <- rgamma(n,shape=1,rate = lambda^2/2)
beta.marginal <- rnorm(n,0,sqrt(tau.sq))
plot(density(beta.marginal), main=TeX(paste("$\\lambda^2 = 2$")), xlim=c(-5,5))
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part c.}]
Consider $p = 1$. Add a hyper prior on $\gamma = 1/\gamma \sim Gamma(a,rate = b)$. Assess how the marginal prior of $\pmb\beta$ changes for $a = 1$ and values of $b \geq 1$.
\end{tcolorbox}

```{r}
set.seed(1)
par(mfrow=c(2,2)) 
rates <- c(1,3,5,10)
for(b in rates){
  lambda <- 1/rgamma(n,1,b)
  tau.sq <- rgamma(n,shape=1,rate = lambda^2/2)
  beta.marginal <- rnorm(n,0,sqrt(tau.sq))
  plot(density(beta.marginal), main=paste("rate b = ",b),xlim=c(-5,5))
}
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part d.}]
Considering the hyper prior in \textbf{c.}, describe a Markov Chain Monte Carlo algorithm to sample from the posterior distribution of $\pmb\beta$ and $\sigma^2$.
\end{tcolorbox}

I will implement a joint Gibbs and Metropolis sampler. The model is
$$
\begin{aligned}
\pmb{Y}|\pmb\beta,\sigma^2 &\sim N(\pmb{X\beta},\sigma^2\pmb{I})\\
\beta_j|\tau^2_j &\sim N(0,\tau^2_j)\\
\tau^2_j &\sim \text{Gamma}(1,\frac{\lambda^2}{2})\\
\lambda^2 &\sim \text{Inverse-Gamma}(a,1/b)\\
\sigma^2 &\sim \text{Inverse-Gamma}(0.1,0.1).
\end{aligned}
$$
I need the full conditionals
$$
\begin{aligned}
\{\beta_1,\ldots,\beta_p| \pmb{Y},\sigma^2,\tau_1^2,\ldots,\tau_p^2, \lambda\},\\
\{\sigma^2| \pmb{Y},\beta_1,\ldots,\beta_p,\tau_1^2,\ldots,\tau_p^2, \lambda\},\\
\{\tau_1^2,\ldots,\tau^2_p| \pmb{Y},\beta_1,\ldots,\beta_p,\sigma^2, \lambda\},\\
\{\lambda| \pmb{Y},\beta_1,\ldots,\beta_p,\sigma^2,\tau_1^2,\ldots,\tau_p^2\}
\end{aligned}
$$
which are all proportional to
$$
p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)
\times p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)
\times p(\tau^2_1,\ldots,\tau^2_p|\lambda)p(\lambda)p(\sigma^2)
$$

so I'll start with the posterior
$$
\begin{aligned}
p(\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda|\pmb{Y})&\propto p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)\\
&\qquad \times p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)\\
&\qquad \times p(\tau^2_1,\ldots,\tau^2_p|\lambda)p(\lambda)p(\sigma^2).
\end{aligned}
$$
As a function of just $\sigma^2$, this is proportional to
$$
\begin{aligned}
  &p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)p(\sigma^2)\\
  =&\quad N(\pmb{X\beta},\sigma^2\pmb I)IG(a,b).
\end{aligned}
$$
Time to show this is inverse-gamma distributed.
$$
\begin{aligned}
&\quad N(\pmb y;\pmb{X\beta},\sigma^2\pmb I)IG(\sigma^2;q,r)\\
\propto \ &(\sigma^2)^{-n/2}\exp\big\{-\frac{1}{2\sigma^2}(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})\big\}(\sigma^2)^{q-1}\exp\big\{-\frac{r}{\sigma^2}\big\}\\
= \ &(\sigma^2)^{-(n/2+q)-1}\exp\big\{-\frac{1}{\sigma^2}(r+\frac{1}{2}(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})\big\}\\
= \ & IG(n/2+q, r+(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})/2)|_{q=0.1,r=0.1}
\end{aligned}
$$
As a function of $\pmb\beta$, the conditional is proportional to
$$
\begin{aligned}
  &p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)
  p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)\\
  =&\quad N(\pmb{X\beta},\sigma^2\pmb I)\cdot\prod_{i=1}^pN(0,\tau_i^2)\\
  =& \quad N(\pmb X\beta,\Sigma)\cdot N(0,\Omega),\text{ where }\Omega=\text{diag}(\tau_1^2,\ldots,\tau_p^2)\\
  =& \quad N(m,\pmb M)
\end{aligned}
$$
with $m=\pmb M\pmb X^\top\Sigma^{-1}y$ and $\pmb M=(\pmb X^\top\Sigma^{-1}\pmb X+\Omega^{-1})^{-1}$.
As a function of $\tau_1^2,\ldots,\tau^2_p$, the conditional is non-standard, but it's proportional to
$$
\begin{aligned}
  &p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)p(\tau_1^2,\ldots,\tau_p^2|\lambda)\\
  =&\quad\prod_{i=1}^pN(0,\tau_i^2)\cdot\prod_{i=1}^p IG(1,\frac{\lambda^2}{2})
\end{aligned}
$$
Finally, as a function of $\lambda$, it's proportional to
$$
\begin{aligned}
  &p(\tau_1^2,\ldots,\tau_p^2|\lambda)p(\lambda)\\
  =&\quad\prod_{i=1}^pIG(1,\frac{\lambda^2}{2})\cdot IG(0.1,0.1)\\
\end{aligned}
$$
Now I can build an algorithm to iteratively update through these conditional distributions.
I take the starting value of $\pmb\beta^{(0)}$ to be the least-squares solution $\pmb{\hat\beta}$.

\begin{algorithm}
\DontPrintSemicolon
\setstretch{1.5}
  \KwResult{Samples from joint posterior $p(\pmb\beta,\sigma^2|\pmb y)$ }
  \For{s in \# samples}{
    $\sigma^{2(s+1)}\sim IG(n/2+a, 2b+(\pmb{y}-\pmb{X\beta}^{(s)})^\top(\pmb{y}-\pmb{X\beta}^{(s)})/2)$\;
    $\pmb\beta^{s}\sim N_p(m,\pmb M)$\;
    $m^\top=\qquad\qquad \pmb M=$
    $\tau_1^2,\ldots,\tau_p^2 \sim N(\pmb\tau^{(s)},\delta\pmb I)^2$\;
    
    
    
    
    $\lambda^{*}\sim N(\lambda.s,\delta)^2$
    
    
    
    
    $\log(r)=\log p(\pmb y|\pmb\beta^{*},\tau_1^2,\ldots,\tau_p^2,\sigma^{2(s)})+\log p(\pmb\beta^{*}|\tau^2_1,\ldots,\tau^2_p)$\; 
    $\qquad\qquad\qquad-\log p(\pmb y|\pmb\beta^{(s)},\tau_1^2,\ldots,\tau_p^2,\sigma^{2(s)})-\log p(\pmb\beta^{(s)}|\tau^2_1,\ldots,\tau^2_p)$\;
    $u\sim Unif(0,1)$\;
    \eIf{$\log(u)<\log(r)$}{
        $\pmb\tau^{(s+1)}=\pmb\beta^*$
    }{
        $\pmb\tau^{(s+1)}=\pmb\beta^{(s)}$
    }
    }   
  \caption{Gibbs and Metropolis}
\end{algorithm}


\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part f.}]
Implement such algorithm in \textsf{R} and compare your results with estimates obtained using \textbf{glmnet()}. In particular, you should test your results on the diabetes data available from lars, (use the matrix of predictors x).
\end{tcolorbox}



```{r, eval=F,warning=F}
loglambda.target <- function(lambda, a, b, tau2){
  (-a - 1)*log(lambda) - (lambda^2/2 * sum(tau2)) - 1/(b*lambda)
}
logtau2j.target <- function(tau2j, lambda, betaj){
  -log(sqrt(tau2j)) - 1/2*(1/tau2j*betaj^2 + lambda^2*tau2j)
}

set.seed(1)
data("diabetes")
X <- cbind(diabetes$x); y <- diabetes$y; n <- nrow(X)
X <- cbind(rep(1,n),X)
p <- ncol(X)
samples <- 50000;
#lambda <- 1e-10
lambda <- 1
tau2 <- rep(1000,p)
beta <- solve(t(X)%*%X)%*%t(X)%*%y
sigma2.keep <- lambda.keep <-rep(0,samples)
beta.keep <-tau2.keep<- matrix(NA,nrow=p,ncol=samples)
sigma2 <-var(y)
sigma2.keep[1] <- sigma2
beta.keep[,1] <- solve(t(X)%*%X)%*%t(X)%*%y
lambda.keep[1] <- lambda
tau2.keep[,1] <- tau2
s <- 2
post.means <- matrix(NA,nrow=p,ncol=length(lambdas))
index <- 1
b<-1
for(s in 2:samples){
  lambda.p <- exp(log(lambda)+rnorm(1,0,lambda))
  logr <- loglambda.target(lambda.p,1,b,tau2)-loglambda.target(lambda,1,b,tau2)+log(lambda.p)-log(lambda)
  if(log(runif(1))<logr){lambda <- lambda.p}
  lambda.keep[s] <- lambda
  for(j in 1:length(tau2)){
    tau2j.p <- exp(log(tau2[j])+rnorm(1,0,5))
    logr <-
      logtau2j.target(tau2j.p, lambda, beta[j]) -
      logtau2j.target(tau2[j], lambda, beta[j]) +
      log(tau2j.p)-log(tau2[j])
    if(log(runif(1))<logr){tau2[j] <- tau2j.p}
  }
  # tau2 <- 1/rinvgauss(n = length(beta), mean = lambda/abs(beta),
  #                        shape = lambda^2)
  sigma2 <- rinvgamma(1,shape=n/2+0.1,rate=(0.1+t(y-X%*%beta)%*%(y-X%*%beta)/2))
  M <- solve(t(X)%*%X*1/sigma2+diag(1/tau2))
  m <- M%*%t(X)%*%y/sigma2
  beta <- t(rmvnorm(n=1,mean=m,sigma=M))
  sigma2.keep[s] <- sigma2
  beta.keep[,s] <- beta
  tau2.keep[,s] <- tau2
}
post.means[,index] <- matrix(rowMeans(beta.keep[,floor(samples/4):samples]),nrow = p)
index <- index +1
}
fit <- glmnet(X, y)
glmnetCoef <- coef(fit,s=1)
plot(fit, xvar="lambda")

fit <- glmnet(X, y)
glmnetCoef <- coef(fit,s=1)
# df <- cbind(beta.ls,colMeans(beta.keep[floor(samples/4):samples,]),glmnetCoef[-1,])
# colnames(df) <- c("Least Square", "Bayes", "Glmnet")
# df
plot(log(lambda.keep[1:samples]), type="l")
beta.ls <- solve(t(X)%*%X)%*%t(X)%*%y
cbind(beta.ls,betaChain)
beta.keep <- t(beta.keep)
tau2.keep <- t(tau2.keep)
plot(sigma2.keep[1:samples],type="l")
plot(log(tau2.keep[,1]),type="l")
plot((tau2.keep[,2]),type="l")
plot((tau2.keep[,3]),type="l")
plot((tau2.keep[,4]),type="l")
plot((tau2.keep[,5]),type="l")
plot((tau2.keep[,6]),type="l")
plot((tau2.keep[,7]),type="l")
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part g.}]
Free $\lambda$ and carry out a sensitivity analysis assessing the behavior of the posterior distribution of $\pmb\beta$ and $\sigma^2$, as hyper parameters a and b are changed. Explain clearly the rationale you use to assess sensitivity and provide recommendations for the analysis of the diabetes data.
\end{tcolorbox}
```{r, eval=F,warning=F}
loglambda.target <- function(lambda, a, b, tau2){
  (-a - 1)*log(lambda) - (lambda^2/2 * sum(tau2)) - 1/(b*lambda)
}
logtau2j.target <- function(tau2j, lambda, betaj){
  -log(sqrt(tau2j)) - 1/2*(1/tau2j*betaj^2 + lambda^2*tau2j)
}

set.seed(1)
data("diabetes")
X <- cbind(diabetes$x); y <- diabetes$y; n <- nrow(X)
X <- cbind(rep(1,n),X)
p <- ncol(X)
samples <- 5000;
#lambda <- 1e-10
lambda <- 1
tau2 <- rep(1000,p)
beta <- solve(t(X)%*%X)%*%t(X)%*%y
sigma2.keep <- lambda.keep <-rep(0,samples)
beta.keep <-tau2.keep<- matrix(NA,nrow=p,ncol=samples)
sigma2 <-var(y)
sigma2.keep[1] <- sigma2
beta.keep[,1] <- solve(t(X)%*%X)%*%t(X)%*%y
lambda.keep[1] <- lambda
tau2.keep[,1] <- tau2
s <- 2
lambdas <- seq(from=-12, to = -1, length.out = 12)
post.means <- matrix(NA,nrow=p,ncol=length(lambdas))
index <- 1
for(l in lambdas){
for(s in 2:samples){
  # lambda.p <- exp(log(lambda)+rnorm(1,0,lambda))
  # logr <- loglambda.target(lambda.p,1,b,tau2)-loglambda.target(lambda,1,b,tau2)+log(lambda.p)-log(lambda)
  # if(log(runif(1))<logr){lambda <- lambda.p}
  # lambda.keep[s] <- lambda
  lambda <- exp(l)
  j<-1
  for(j in 1:length(tau2)){
    tau2j.p <- exp(log(tau2[j])+rnorm(1,0,5))
    logr <-
      logtau2j.target(tau2j.p, lambda, beta[j]) -
      logtau2j.target(tau2[j], lambda, beta[j]) +
      log(tau2j.p)-log(tau2[j])
    if(log(runif(1))<logr){tau2[j] <- tau2j.p}
  }
  # tau2 <- 1/rinvgauss(n = length(beta), mean = lambda/abs(beta), shape = lambda^2)
  sigma2 <- rinvgamma(1,shape=n/2+0.1,rate=(0.1+t(y-X%*%beta)%*%(y-X%*%beta)/2))
  M <- solve(t(X)%*%X*1/sigma2+diag(1/tau2))
  m <- M%*%t(X)%*%y/sigma2
  beta <- t(rmvnorm(n=1,mean=m,sigma=M))
  sigma2.keep[s] <- sigma2
  beta.keep[,s] <- beta
  tau2.keep[,s] <- tau2
}
post.means[,index] <- matrix(rowMeans(beta.keep[,floor(samples/4):samples]),nrow = p)
index <- index +1
}
fit <- glmnet(X, y)
glmnetCoef <- coef(fit,s=1)
plot(fit, xvar="lambda")
betaChain <- colMeans(beta.keep[,1:samples])

plot(lambdas,post.means[2,],ylab="Coefficients",xlab="log lambdas",ylim=c(min(post.means),max(post.means)),type="l")
lines(lambdas,post.means[2,])
for(j in 3:p){
  points(lambdas,post.means[j,],ylab="Coefficients",xlab="log lambdas", col=j,type="l")
lines(lambdas,post.means[j,], col=j)
}

fit <- glmnet(X, y)
plot(fit,xvar = "lambda")
glmnetCoef <- coef(fit,s=1)
# df <- cbind(beta.ls,colMeans(beta.keep[floor(samples/4):samples,]),glmnetCoef[-1,])
# colnames(df) <- c("Least Square", "Bayes", "Glmnet")
# df
beta.ls <- solve(t(X)%*%X)%*%t(X)%*%y
sajd <- glmnetCoef
cbind(beta.ls,post.means,coef(fit,s=1)[-2,])
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part h.}]
Implementation and benchmarking in Julia
\end{tcolorbox}

```{r, eval=F,warning=F}
loglambda.target <- function(lambda, a, b, tau2){
  (-a - 1)*log(lambda) - (lambda^2/2 * sum(tau2)) - 1/(b*lambda)
}
logtau2j.target <- function(tau2j, lambda, betaj){
  -log(sqrt(tau2j)) - 1/2*(1/tau2j*betaj^2 + lambda^2*tau2j)
}
bs <- c(seq(from=0.0001,to=1.5,length.out = 10),seq(from=1,to=20,length.out = 10))
set.seed(1)
data("diabetes")
X <- cbind(diabetes$x); y <- diabetes$y; n <- nrow(X)
X <- cbind(rep(1,n),X)
p <- ncol(X)
betasB <- matrix(0,nrow=p,ncol=length(bs))
samples <- 5000;
#lambda <- 1e-10
lambda <- 1
tau2 <- rep(1000,p)
beta <- solve(t(X)%*%X)%*%t(X)%*%y
sigma2.keep <- lambda.keep <-rep(0,samples)
beta.keep <-tau2.keep<- matrix(NA,nrow=p,ncol=samples)
sigma2 <-var(y)
sigma2.keep[1] <- sigma2
beta.keep[,1] <- solve(t(X)%*%X)%*%t(X)%*%y
lambda.keep[1] <- lambda
tau2.keep[,1] <- tau2
s <- 2
lambdas <- seq(from=-12, to = -2, length.out = 12)
post.means <- matrix(NA,nrow=p,ncol=length(lambdas))
index <- 1
bindex <- 1
for(b in bs){
for(s in 2:samples){
  lambda.p <- exp(log(lambda)+rnorm(1,0,lambda))
  logr <- loglambda.target(lambda.p,1,b,tau2)-loglambda.target(lambda,1,b,tau2)+log(lambda.p)-log(lambda)
  if(log(runif(1))<logr){lambda <- lambda.p}
  lambda.keep[s] <- lambda
  for(j in 1:length(tau2)){
    tau2j.p <- exp(log(tau2[j])+rnorm(1,0,5))
    logr <-
      logtau2j.target(tau2j.p, lambda, beta[j]) -
      logtau2j.target(tau2[j], lambda, beta[j]) +
      log(tau2j.p)-log(tau2[j])
    if(log(runif(1))<logr){tau2[j] <- tau2j.p}
  }
  # tau2 <- 1/rinvgauss(n = length(beta), mean = lambda/abs(beta),
  #                        shape = lambda^2)
  sigma2 <- rinvgamma(1,shape=n/2+0.1,rate=(0.1+t(y-X%*%beta)%*%(y-X%*%beta)/2))
  M <- solve(t(X)%*%X*1/sigma2+diag(1/tau2))
  m <- M%*%t(X)%*%y/sigma2
  beta <- t(rmvnorm(n=1,mean=m,sigma=M))
  sigma2.keep[s] <- sigma2
  beta.keep[,s] <- beta
  tau2.keep[,s] <- tau2
}
post.means[,index] <- matrix(rowMeans(beta.keep[,floor(samples/4):samples]),nrow = p)
post.meansindex <- index +1
betasB[,bindex] <- rowMeans(beta.keep[,1000:samples])
bindex <- bindex+1
}


# df <- cbind(beta.ls,colMeans(beta.keep[floor(samples/4):samples,]),glmnetCoef[-1,])
# colnames(df) <- c("Least Square", "Bayes", "Glmnet")
# df
plot(bs,betasB[2,],ylim = c(min(betasB),max(betasB)),type="l")
for(j in 3:length(bs)){
  points(bs,betasB[j,],ylim = c(min(betasB[j,]),max(betasB[j,])), type="l", col=j)
}
```





