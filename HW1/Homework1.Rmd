---
title: "Homework 1"
author: "Ian Frankenburg"
date: "4/7/2020"
header-includes:
   - \usepackage{bm}
   - \usepackage{algorithmic}
   - \usepackage[]{algorithm2e}
   - \usepackage{tikz,lipsum,lmodern}
   - \usepackage[most]{tcolorbox}
   - \usepackage{setspace}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
library(latex2exp)
library(mvtnorm)
library(invgamma)
library(lars)
library(matrixStats)
library(statmod)
library(glmnet)
```

# Bayesian Adaptive Lasso
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part a.}]
Consider $p = 1$. Simulate 5,000 Monte Carlo samples from the conditional prior $\pmb\beta | \tau^2 = 1$ and obtain a plot of the density using the \textsf{R} function density.
\end{tcolorbox}
```{r,fig.height = 3.5, fig.width = 3.5, fig.align = "center"}
n <- 5000
plot(density(rnorm(n,0,1)), main=TeX(paste("$\\beta$", "marginal")))
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part b.}]
Consider $p = 1$. Simulate 5,000 Monte Carlo samples from the marginal prior $\pmb\beta$, considering $\lambda^2 = 2$, so that $\mathbb E(\tau^2|\lambda) = 1$. Obtain a plot of the density as in \textbf{a.}
\end{tcolorbox}

```{r, fig.height = 3.5, fig.width = 3.5, fig.align = "center"}
lambda <- sqrt(2)
tau.sq <- rgamma(n,shape=1,rate = lambda^2/2)
beta.marginal <- rnorm(n,0,sqrt(tau.sq))
plot(density(beta.marginal), main=TeX(paste("$\\lambda^2 = 2$")), xlim=c(-5,5))
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part c.}]
Consider $p = 1$. Add a hyper prior on $\gamma = 1/\gamma \sim Gamma(a,rate = b)$. Assess how the marginal prior of $\pmb\beta$ changes for $a = 1$ and values of $b \geq 1$.
\end{tcolorbox}

```{r}
set.seed(1)
par(mfrow=c(2,2)) 
rates <- c(1,3,5,10)
for(b in rates){
  lambda <- 1/rgamma(n,1,b)
  tau.sq <- rgamma(n,shape=1,rate = lambda^2/2)
  beta.marginal <- rnorm(n,0,sqrt(tau.sq))
  plot(density(beta.marginal), main=paste("rate b = ",b),xlim=c(-5,5))
}
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part d.}]
Considering the hyper prior in \textbf{c.}, describe a Markov Chain Monte Carlo algorithm to sample from the posterior distribution of $\pmb\beta$ and $\sigma^2$.
\end{tcolorbox}

I will implement a joint Gibbs and Metropolis sampler. The model is
$$
\begin{aligned}
\pmb{Y}|\pmb\beta,\sigma^2 &\sim N(\pmb{X\beta},\sigma^2\pmb{I})\\
\beta_j|\tau^2_j &\sim N(0,\tau^2_j)\\
\tau^2_j &\sim \text{Gamma}(1,\frac{\lambda^2}{2})\\
\lambda^2 &\sim \text{Inverse-Gamma}(a,1/b)\\
\sigma^2 &\sim \text{Inverse-Gamma}(0.1,0.1).
\end{aligned}
$$
I need the full conditionals
$$
\begin{aligned}
\{\beta_1,\ldots,\beta_p| \pmb{Y},\sigma^2,\tau_1^2,\ldots,\tau_p^2, \lambda\},\\
\{\sigma^2| \pmb{Y},\beta_1,\ldots,\beta_p,\tau_1^2,\ldots,\tau_p^2, \lambda\},\\
\{\tau_1^2,\ldots,\tau^2_p| \pmb{Y},\beta_1,\ldots,\beta_p,\sigma^2, \lambda\},\\
\{\lambda| \pmb{Y},\beta_1,\ldots,\beta_p,\sigma^2,\tau_1^2,\ldots,\tau_p^2\}
\end{aligned}
$$
which are all proportional to
$$
p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)
\times p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)
\times p(\tau^2_1,\ldots,\tau^2_p|\lambda)p(\lambda)p(\sigma^2)
$$

so I'll start with the posterior
$$
\begin{aligned}
p(\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda|\pmb{Y})&\propto p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)\\
&\qquad \times p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)\\
&\qquad \times p(\tau^2_1,\ldots,\tau^2_p|\lambda)p(\lambda)p(\sigma^2).
\end{aligned}
$$
As a function of just $\sigma^2$, this is proportional to
$$
\begin{aligned}
  &p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)p(\sigma^2)\\
  =&\quad N(\pmb{X\beta},\sigma^2\pmb I)IG(a,b).
\end{aligned}
$$
Time to show this is inverse-gamma distributed.
$$
\begin{aligned}
&\quad N(\pmb y;\pmb{X\beta},\sigma^2\pmb I)IG(\sigma^2;q,r)\\
\propto \ &(\sigma^2)^{-n/2}\exp\big\{-\frac{1}{2\sigma^2}(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})\big\}(\sigma^2)^{q-1}\exp\big\{-\frac{r}{\sigma^2}\big\}\\
= \ &(\sigma^2)^{-(n/2+q)-1}\exp\big\{-\frac{1}{\sigma^2}(r+\frac{1}{2}(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})\big\}\\
= \ & IG(n/2+q, r+(\pmb{y}-\pmb{X\beta})^\top(\pmb{y}-\pmb{X\beta})/2)|_{q=0.1,r=0.1}
\end{aligned}
$$
As a function of $\pmb\beta$, the conditional is proportional to
$$
\begin{aligned}
  &p(\pmb{Y}|\beta_1,\ldots,\beta_p,\tau^2_1,\ldots,\tau^2_p,\sigma^2,\lambda)
  p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)\\
  =&\quad N(\pmb{X\beta},\sigma^2\pmb I)\cdot\prod_{i=1}^pN(0,\tau_i^2)\\
  =& \quad N(\pmb X\beta,\Sigma)\cdot N(0,\Omega),\text{ where }\Omega=\text{diag}(\tau_1^2,\ldots,\tau_p^2)\\
  =& \quad N(m,\pmb M)
\end{aligned}
$$
with $m=\pmb M\pmb X^\top\Sigma^{-1}y$ and $\pmb M=(\pmb X^\top\Sigma^{-1}\pmb X+\Omega^{-1})^{-1}$.
As a function of $\tau_1^2,\ldots,\tau^2_p$, the conditional is non-standard, but it's proportional to
$$
\begin{aligned}
  &p(\beta_1,\ldots,\beta_p|\tau^2_1,\ldots,\tau^2_p)p(\tau_1^2,\ldots,\tau_p^2|\lambda)\\
  =&\quad\prod_{i=1}^pN(0,\tau_i^2)\cdot\prod_{i=1}^p IG(1,\frac{\lambda^2}{2})
\end{aligned}
$$
Finally, as a function of $\lambda$, it's proportional to
$$
\begin{aligned}
  &p(\tau_1^2,\ldots,\tau_p^2|\lambda)p(\lambda)\\
  =&\quad\prod_{i=1}^pIG(1,\frac{\lambda^2}{2})\cdot IG(0.1,0.1)\\
\end{aligned}
$$
Now I can build an algorithm to iteratively update through these conditional distributions.
I take the starting value of $\pmb\beta^{(0)}$ to be the least-squares solution $\pmb{\hat\beta}$.

\begin{algorithm}
\DontPrintSemicolon
\setstretch{1.5}
  \KwResult{Samples from joint posterior $p(\pmb\beta,\sigma^2|\pmb y)$ }
  \For{s in \# samples}{
    $\sigma^{2(s+1)}\sim IG(n/2+a, 2b+(\pmb{y}-\pmb{X\beta}^{(s)})^\top(\pmb{y}-\pmb{X\beta}^{(s)})/2)$\;
    $\pmb\beta^{s}\sim N_p(m,\pmb M)$\;
    $m^\top=\qquad\qquad \pmb M=$
    $\tau_1^2,\ldots,\tau_p^2 \sim N(\pmb\tau^{(s)},\delta\pmb I)^2$\;
    
    
    
    
    $\lambda^{*}\sim N(\lambda.s,\delta)^2$
    
    
    
    
    $\log(r)=\log p(\pmb y|\pmb\beta^{*},\tau_1^2,\ldots,\tau_p^2,\sigma^{2(s)})+\log p(\pmb\beta^{*}|\tau^2_1,\ldots,\tau^2_p)$\; 
    $\qquad\qquad\qquad-\log p(\pmb y|\pmb\beta^{(s)},\tau_1^2,\ldots,\tau_p^2,\sigma^{2(s)})-\log p(\pmb\beta^{(s)}|\tau^2_1,\ldots,\tau^2_p)$\;
    $u\sim Unif(0,1)$\;
    \eIf{$\log(u)<\log(r)$}{
        $\pmb\tau^{(s+1)}=\pmb\beta^*$
    }{
        $\pmb\tau^{(s+1)}=\pmb\beta^{(s)}$
    }
    }   
  \caption{Gibbs and Metropolis}
\end{algorithm}

\pagebreak
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part e.}]
temp
\end{tcolorbox}

```{r, eval=F,warning=F}
set.seed(1)
data("diabetes")
X <- cbind(diabetes$x); y <- diabetes$y; n <- nrow(X);p <- ncol(X)
# X <- cbind(rep(1,n),X)
samples <- 5000;
lambda2 <- 0.1
tau2 <- rep(1000,p)
beta <- solve(t(X)%*%X)%*%t(X)%*%y
sigma2.keep <- lambda2.keep <-rep(0,samples)
beta.keep <-tau2.keep <- matrix(NA,nrow=p,ncol=samples)
sigma2 <-var(y)
sigma2.keep[1] <- sigma2
beta.keep[,1] <- solve(t(X)%*%X)%*%t(X)%*%y
lambda2.keep[1] <- lambda2
tau2.keep[,1] <- tau2
s <- 2
lambdas <- c(exp(-20),exp(-10),exp(-9),exp(-7),exp(-6),exp(-5),exp(-4),exp(-3),exp(-2),exp(-1))
post.means <- matrix(NA,nrow=p,ncol=length(lambdas))
index <- 1
for(l in lambdas){
for(s in 2:samples){
  # lambda2.p <- exp(log(lambda2)+rnorm(1,0,.5))
  # logr <- sum(dgamma(tau2,1,lambda2.p/2,log=T))+
  #   dgamma(lambda2.p,1,1,log=T)-
  #   sum(dgamma(tau2,1,lambda2/2,log=T))-
  #   dgamma(lambda2,1,1,log=T)+
  #   log(lambda2.p)-log(lambda2)
  # if(log(runif(1))<logr){lambda2 <- lambda2.p}
  lambda2 <- l
  for(i in 1:p){
    tau2i.p <- exp(log(tau2[i])+rnorm(1,0,0.1))
    logr <- dnorm(beta[i],0,tau2i.p,log=T)+
      dgamma(tau2i.p,1,lambda2/2,log=T)-
      dnorm(beta[i],0,tau2[i],log=T)-
      dgamma(tau2[i],1,lambda2/2,log=T)+
      log(tau2i.p)-log(tau2[i])
    if(log(runif(1))<logr){tau2[i] <- tau2i.p}
  }
  # tau2 <- 1/rinvgauss(n = length(beta), mean = lambda2/abs(beta), 
  #                       shape = lambda2)
  sigma2 <- rinvgamma(1,shape=n/2+0.1,rate=(0.1+t(y-X%*%beta)%*%(y-X%*%beta)/2))
  M <- solve(t(X)%*%X*1/sigma2+diag(1/tau2))
  m <- M%*%t(X)%*%y/sigma2
  beta <- t(rmvnorm(n=1,mean=m,sigma=M))
  sigma2.keep[s] <- sigma2
  beta.keep[,s] <- beta
  tau2.keep[,s] <- tau2
  lambda2.keep[s] <- lambda2
}
post.means[,index] <- matrix(rowMeans(beta.keep[,floor(samples/4):samples]),nrow = p)
index <- index +1
}
fit <- glmnet(X, y)
glmnetCoef <- coef(fit,s=1)
df <- cbind(beta.ls,rowMeans(beta.keep[,floor(samples/4):samples]),glmnetCoef[-1,])
plot(fit, xvar="lambda")
plot(log(lambdas),post.means[1,],ylab="Coefficients",xlab="log lambdas",ylim=c(-300,300))
lines(log(lambdas),post.means[1,])
for(j in 2:10){
points(log(lambdas),post.means[j,],ylab="Coefficients",xlab="log lambdas", col=j)
lines(log(lambdas),post.means[j,], col=j)
}



# beta.ls <- solve(t(X)%*%X)%*%t(X)%*%y
# fit <- glmnet(X, y)
# glmnetCoef <- coef(fit,s=1)
# df <- cbind(beta.ls,rowMeans(beta.keep[,floor(samples/4):samples]),glmnetCoef[-1,])
# colnames(df) <- c("Least Square", "Bayes", "Glmnet")
# df
plot(lambda2.keep[1000:samples], type="l")
beta.keep <- t(beta.keep)
tau2.keep <- t(tau2.keep)
plot(sigma2.keep[1000:samples],type="l")
plot((tau2.keep[,1]),type="l")
plot((tau2.keep[,2]),type="l")
plot((tau2.keep[,3]),type="l")
plot((tau2.keep[,4]),type="l")
plot((tau2.keep[,5]),type="l")
plot((tau2.keep[,6]),type="l")
plot((tau2.keep[,7]),type="l")
# plot(beta.keep[,1],type="l")
# plot(beta.keep[,2],type="l")
# plot(beta.keep[,3],type="l")
# plot(beta.keep[,4],type="l")
# plot(beta.keep[,5],type="l")
# plot(beta.keep[,6],type="l")
# plot(beta.keep[,7],type="l")
```


\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part f.}]
Implement such algorithm in \textsf{R} and compare your results with estimates obtained using \textbf{glmnet()}. In particular, you should test your results on the diabetes data available from lars, (use the matrix of predictors x).
\end{tcolorbox}

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part g.}]
Free $\lambda$ and carry out a sensitivity analysis assessing the behavior of the posterior distribution of $\pmb\beta$ and $\sigma^2$, as hyper parameters a and b are changed. Explain clearly the rationale you use to assess sensitivity and provide recommendations for the analysis of the diabetes data.
\end{tcolorbox}

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part h.}]
Implementation and benchmarking in Julia
\end{tcolorbox}







