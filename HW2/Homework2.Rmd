---
title: "Homework 2"
author: "Ian Frankenburg"
date: "5/8/2020"
header-includes:
   - \usepackage{bm}
   - \usepackage{algorithmic}
   - \usepackage{algorithm2e}
   - \usepackage{tikz,lipsum,lmodern}
   - \usepackage[most]{tcolorbox}
   - \usepackage{setspace}
   - \usepackage{cancel}
   
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
set.seed(123)
library(kableExtra)
library(latex2exp)
library(mvtnorm)
library(invgamma)
library(lars)
library(matrixStats)
library(statmod)
library(glmnet)
library(tidyverse)
library(dplyr)
library(tidyr)
library(Rcpp)
```
# Generalized Linear Models
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (1).}]
Describe and implement a Metropolis-Hastings algorithm designed to obtain a MC with stationary distribution $p(\beta|Y)$
\end{tcolorbox}
This MCMC implementation is pretty strightforward. I know my target $p(\pmb\beta|\pmb y)$ is propportional to 
$$
\Big[\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}\Big]\exp\Big(-\frac{1}{2n}\pmb\beta^\top(X^\top X\pmb)\pmb\beta\Big),\text{ where } p_i =\Phi(x_i^\top\pmb\beta).
$$
My Metropolis algorithm will make symmetric proposals based around the current iteration of $\pmb\beta$ with variance defined to be the inverse sample covariance $n(X^\top X)^{-1}:$
$$
\pmb\beta^*\sim N(\pmb\beta^{(s)},n(X^\top X)^{-1}).
$$

```{r, fig.align="center"}
require(survival)
data <- infert
target <- function(y, x, xtx, beta, n){
  p <- pnorm(x%*%beta)
  return(sum(y*log(p)+(1-y)*log(1-p))-1/(2*n)*t(beta)%*%xtx%*%beta)
}
samples <- 5000
y <- data$case
n <- length(y)
data$education <- as.numeric(as.factor(data$education))
edu1 <- data$education==2; edu2 <- data$education==3
x <- cbind(rep(1,n),data$age, data$parity, edu1, edu2, data$spontaneous, data$induced)
ncolx <- ncol(x)
xtx <- t(x)%*%x
xtx_inv <- solve(xtx)
beta <- t(rmvnorm(1,rep(0,ncol(x)),xtx_inv*n))
beta.chain <- matrix(0,nrow=ncolx,ncol=samples)
beta.chain[,1] <- beta
for(s in 2:samples){
  beta_new <- t(rmvnorm(1,as.vector(beta), xtx_inv))
  logr <- target(y,x,xtx,beta_new,n)-target(y,x,xtx,beta,n)
  if(log(runif(1)) < logr){beta <- beta_new}
  beta.chain[,s] <- beta
}
```

```{r, echo=F}
par(mfrow=c(3,3))
plot(beta.chain[1,],type="l")
plot(beta.chain[2,],type="l")
plot(beta.chain[3,],type="l")
plot(beta.chain[4,],type="l")
plot(beta.chain[5,],type="l")
plot(beta.chain[6,],type="l")
plot(beta.chain[7,],type="l")
df <- rbind(
  "Bayes" = rowMeans(beta.chain),
  "GLM" = coef(glm(y~x[,-1],family=binomial(link="probit")))
)
df
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (2).}]
Describe and implement a data augmented (DA-MCMC) strategy targeting $p(\beta|Y)$
\end{tcolorbox}
The original model is 
$$
P(y_i=1|x_i,\beta)=\Phi(x_i^\top\beta).
$$
This is equivalent to the model $P(y_i=1|x_i,\beta)=P(\pmb 1(z_i>0)=1)=P(z_i>0)$, where $z_i\stackrel{\text{iid}}{\sim} N(x_i^\top\beta,1)$.
This follows immediately since 
$$
\int_{-\infty}^{x_i^\top\beta} N(t;\ 0, 1)dt=\int_{-\infty}^{0}N(z_i;x_i^\top\beta,1)dz_i
$$
by a change of variables $z_i:=t-x_i^\top\beta$. Thus $\Phi(x_i^\top\beta)=P(z_i>0)$.

In defining the latent model, the full conditionals $p(\beta|\pmb y, z)$, $p(\pmb z|\pmb y,\pmb \beta)$ become tractible, so I can use a Gibbs sampler.

I'll start with $p(\pmb\beta|\pmb y, \pmb z)$
$$
\begin{aligned}
p(\pmb\beta|\pmb y, \pmb z)&\propto p(\pmb y, \pmb z|\pmb\beta)p(\pmb\beta)\\
&=\xcancel{p(\pmb y|\pmb\beta, \pmb z)}p(\pmb z|\pmb \beta)p(\pmb\beta)=p(\pmb z|\pmb \beta)p(\pmb\beta)\\
&=N(\pmb z;X\pmb\beta,\pmb I)N(\pmb\beta;\pmb 0,n(X^\top X)^{-1})\\
&=N(\frac{n}{n+1}(X^\top X)^{-1}X^\top\pmb y,\frac{n}{n+1}(X^\top X)^{-1})
\end{aligned}
$$

Now for $p(\pmb z|\pmb y, \pmb \beta)$.
$$
\begin{aligned}
p(\pmb z|\pmb y, \pmb \beta)&\propto p(\pmb y, \pmb \beta, \pmb z)\\
&\propto p(\pmb y|\pmb\beta,\pmb z)p(\pmb z|\pmb\beta)\xcancel{p(\pmb\beta)}\\
&\propto\prod_{i=1}^n[\pmb 1(y_i=1)\pmb 1(z_i>0)+\pmb 1(y_i=0)\pmb 1(z_i<0)]N(\pmb z;X\pmb \beta,\pmb I)\\
\end{aligned}
$$
Since our sampling model assumes the $y$'s are independent, so are the $z$'s and I can sample the full conditionals independently, i.e.
$$
\begin{aligned}
p(z_i|\pmb y, \pmb \beta)&\propto p(\pmb y, \pmb \beta, z_i)\propto p(y|\pmb\beta,z_i)p(z_i\pmb|\beta)\\
&\propto1(y_i=1)\pmb 1(z_i>0)+\pmb 1(y_i=0)\pmb 1(z_i<0)]N(z_i;x_i^\top\pmb \beta,1)\\
&=\begin{cases} 
      N(z_i;x_i^\top\pmb \beta,1)*\pmb1_{[0,\infty)}(z_i)  & \text{if }y_i=1\\
      N(z_i;x_i^\top\pmb \beta,1)*\pmb1_{(-\infty,0)}(z_i) & \text{if }y_i=0\\
   \end{cases}
\end{aligned}
$$
Now I can implement a Gibbs sampler to iteratively draw from these conditionals.


```{r, fig.align="center", warning=F}
require(truncnorm)
samples <- 5000
data <- infert
y <- data$case
n <- length(y)
data$education <- as.numeric(as.factor(data$education))
edu1 <- data$education==2; edu2 <- data$education==3
x <- cbind(rep(1,n),data$age, data$parity, edu1, edu2, data$spontaneous, data$induced)
p <- ncol(x)
xtx <- t(x)%*%x
xtx_inv <- solve(xtx)
beta <- t(rmvnorm(1,rep(0,ncol(x)),xtx_inv*n))
beta.chain <- matrix(0,nrow=p,ncol=samples)
beta.chain[,1] <- beta
z <- t(rmvnorm(1,x%*%beta,sigma=diag(n)))
for(s in 2:samples){
  beta <- t(rmvnorm(1, n/(n+1)*xtx_inv%*%t(x)%*%z, n/(n+1)*xtx_inv))
  # truncated normal
  for(i in 1:n){
    if(y[i]==1){
      z[i] <- rtruncnorm(1, a = 0, b = Inf, mean = matrix(x[i,],ncol=p)%*%beta, sd=1)
    }else{
      z[i] <- rtruncnorm(1, a = -Inf, b = 0, mean = matrix(x[i,],ncol=p)%*%beta, sd=1)
    }
  }
  beta.chain[,s] <- beta
}
```

```{r, echo=F, fig.align="center", warning=F}
par(mfrow=c(3,3))
plot(beta.chain[1,],type="l")
plot(beta.chain[2,],type="l")
plot(beta.chain[3,],type="l")
plot(beta.chain[4,],type="l")
plot(beta.chain[5,],type="l")
plot(beta.chain[6,],type="l")
plot(beta.chain[7,],type="l")
df <- rbind(
  "Bayes" = rowMeans(beta.chain),
  "GLM" = coef(glm(y~x[,-1],family=binomial(link="probit")))
)
df
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (3).}]
Describe and implement a parameter expanded-data augmentation (PX-DA MCMC) algorithm targeting $p(\beta|Y)$.
\end{tcolorbox}
For this parameter-expanded model, I'll introduce another parameter $\alpha^2\sim IG(a,b)$ and I'll consider the transformation $w_i:=\alpha z_i$. Then I'll use a Gibbs sampler to sample iteratively from the conditionals
$$
\begin{aligned}
p(\pmb\beta|\pmb y,\pmb w,\alpha)&\propto\prod_{i=1}^n N(w_i;\alpha x_i^\top\pmb\beta,\alpha^2)N(\pmb\beta;0,n(X^\top X)^{-1})\\
&=N(Mm,M),\text{ where } M=\frac{n}{n+1}(X^\top X)^{-1}\text{ and } m=MX^\top\pmb w/\alpha\\
p(w_i|\pmb y,\pmb \beta,\alpha)&=\begin{cases} 
      N(w_i;\alpha x_i^\top\pmb \beta,\alpha^2)*\pmb1_{[0,\infty)}(w_i)  & \text{if }y_i=1\\
      N(w_i;\alpha x_i^\top\pmb \beta,\alpha^2)*\pmb1_{(-\infty,0)}(w_i) & \text{if }y_i=0\\
   \end{cases}\\
p(\alpha^2|\pmb y,\pmb \beta,\pmb z)&=\prod_{i=1}^n N(w_i;\alpha x_i^\top\pmb\beta, \alpha^2)IG(\alpha^2;a,b)
\end{aligned}
$$

```{r, fig.align="center"}
require(truncnorm)
samples <- 5000
data <- infert
y <- data$case
data$education <- as.numeric(as.factor(data$education))
edu1 <- data$education==2; edu2 <- data$education==3
x <- cbind(rep(1,n),data$age, data$parity, edu1, edu2, data$spontaneous, data$induced)
n <- nrow(x); p <- ncol(x)
xtx <- t(x)%*%x
xtx_inv <- solve(xtx)
beta <- t(rmvnorm(1,rep(0,ncol(x)),xtx_inv*n))
beta.chain <- matrix(0,nrow=p,ncol=samples)
beta.chain[,1] <- beta
alpha2 <- 1
z <- t(rmvnorm(1,x%*%beta,sigma=diag(n)))
for(s in 2:samples){
  beta <- t(rmvnorm(1, n/(n+1)*xtx_inv%*%t(x)%*%z/(sqrt(alpha2)), n/(n+1)*xtx_inv))
  # truncated normal
  for(i in 1:n){
    if(y[i]==1){
      z[i] <- rtruncnorm(1, a = 0, b = Inf, mean = matrix(x[i,],ncol=p)%*%beta, sd=1)
    }else{
      z[i] <- rtruncnorm(1, a = -Inf, b = 0, mean = matrix(x[i,],ncol=p)%*%beta, sd=1)
    }
  }
  rss <- t((z-x%*%beta))%*%(z-x%*%beta)/2
  d <- rchisq(1, df=n)
  alpha2 <- as.numeric(sqrt(rss/d))
  beta.chain[,s] <- beta
}
```

```{r, echo=F, fig.align="center"}
par(mfrow=c(3,3))
plot(beta.chain[1,],type="l")
plot(beta.chain[2,],type="l")
plot(beta.chain[3,],type="l")
plot(beta.chain[4,],type="l")
plot(beta.chain[5,],type="l")
plot(beta.chain[6,],type="l")
plot(beta.chain[7,],type="l")
df <- rbind(
  "Bayes" = rowMeans(beta.chain),
  "GLM" = coef(glm(y~x[,-1],family=binomial(link="probit")))
)
df
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (6).}]
For logit model, describe and a random walk MH targeting $p(\beta|Y)$
\end{tcolorbox}

Similar to the probit case, the posterior is intractible but can be written as
$$
\begin{aligned}
p(\beta|\pmb y)\propto&\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}p(\beta)\\
=&\prod_{i=1}^n \Big[\frac{e^{x_i^\top\beta}}{1+e^{x_i^\top\beta}}\Big]^{y_i}\Big[1-\frac{e^{x_i^\top\beta}}{1+e^{x_i^\top\beta}}\Big]^{1-y_i}N(\pmb\beta;0,n(X^\top X)^{-1})\\
=&\prod_{i=1}^n\frac{(e^{x_i^\top\beta})^{y_i}}{1+e^{x_i^\top\beta}}N(\pmb\beta;0,n(X^\top X)^{-1})
\end{aligned}
$$
This will be my target in the random walk MH.

```{r, fig.align="center"}
data <- infert
y <- data$case
data$education <- as.numeric(as.factor(data$education))
edu1 <- data$education==2; edu2 <- data$education==3
x <- cbind(rep(1,n),data$age, data$parity, edu1, edu2, data$spontaneous, data$induced)
target <- function(y, x, xtx, beta, n){
  p <- exp(x%*%beta)/(1+exp(x%*%beta))
  return(sum(y*log(p)+(1-y)*log(1-p))-1/(2*n)*t(beta)%*%xtx%*%beta)
}
samples <- 5000
n <- nrow(x); ncolx <- ncol(x)
xtx <- t(x)%*%x
xtx_inv <- solve(xtx)
beta <- t(rmvnorm(1,rep(0,ncol(x)),xtx_inv*n))
beta.chain <- matrix(0,nrow=ncolx,ncol=samples)
beta.chain[,1] <- beta
for(s in 2:samples){
  beta_new <- t(rmvnorm(1,as.vector(beta), xtx_inv))
  logr <- target(y,x,xtx,beta_new,n)-target(y,x,xtx,beta,n)
  if(log(runif(1)) < logr){beta <- beta_new}
  beta.chain[,s] <- beta
}
```

```{r, echo=F, fig.align="center"}
par(mfrow=c(3,3))
plot(beta.chain[1,],type="l")
plot(beta.chain[2,],type="l")
plot(beta.chain[3,],type="l")
plot(beta.chain[4,],type="l")
plot(beta.chain[5,],type="l")
plot(beta.chain[6,],type="l")
plot(beta.chain[7,],type="l")
df <- rbind(
  "Bayes" = rowMeans(beta.chain),
  "GLM" = coef(glm(y~x[,-1],family=binomial(link="logit")))
)
df
```


\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (7).}]
Describe and implement a Langevin-Hastings algorithm designed to obtain a MC with stationary distribution $p(\beta|Y)$
\end{tcolorbox}
The Langevin-Hastings algorithm will work by utilizing the a 2^{nd} order Taylor approximation of the target distribution. That way I can try to match the proposal to the target.

If I was proposing for $\beta$ at iteration $s$, I'd use something like
$$
\pmb\beta^*\sim N\Big(\pmb\beta^{(s)}+\frac{1}{2}\sigma^2\nabla\log\big(p(\pmb y|\pmb\beta^{(s)})p(\pmb\beta^{(s)})\big), \sigma^2\pmb I\Big)
$$

Earlier I showed the poserior is proportional to 
$$
\prod_{i=1}^n\frac{(e^{x_i^\top\beta})^{y_i}}{1+e^{x_i^\top\beta}}N(\pmb\beta;0,n(X^\top X)^{-1})
$$
so the log-posterior is 
$$
\begin{aligned}
&\sum_{i=1}^n y_ix_i^\top\pmb\beta-\log(1+e^{x_i^\top\pmb\beta})-\frac{1}{2n}\beta^\top(X^\top X)\beta\\
\Rightarrow&\nabla\Big\{\sum_{i=1}^n y_ix_i^\top\pmb\beta-\log(1+e^{x_i^\top\pmb\beta})-\frac{1}{2n}\pmb \beta^\top(X^\top X)\beta\Big\}\\
=&\sum_{i=1}^n[y_i -\frac{e^{x_i^\top\pmb \beta}}{1+e^{x_i^\top\pmb \beta}}]x_i^\top-\frac{1}{n}\pmb \beta^\top(X^\top X)
\end{aligned}
$$

```{r}
gradient <- function(y, x, xtx, beta, p){
  n <- length(y)
  return(t(y-p)%*%x-1/n*t(beta)%*%xtx)
}
target <- function(y, x, xtx, beta, n){
  return(sum(y*log(p)+(1-y)*log(1-p))-1/(2*n)*t(beta)%*%xtx%*%beta)
}
data <- infert
y <- data$case
data$education <- as.numeric(as.factor(data$education))
edu1 <- data$education==2; edu2 <- data$education==3
x <- cbind(rep(1,n),data$age, data$parity, edu1, edu2, data$spontaneous, data$induced)
samples <- 5000
n <- nrow(x); ncolx <- ncol(x)
xtx <- t(x)%*%x
xtx_inv <- solve(xtx)
beta <- t(rmvnorm(1,rep(0,ncolx),xtx_inv*n))
beta.chain <- matrix(0,nrow=ncolx,ncol=samples)
beta.chain[,1] <- beta
c <- 0.005
num_accepted <- accept_ratio <- 0
for(s in 2:samples){
  p <- exp(x%*%beta)/(1+exp(x%*%beta))
  grad <- gradient(y, x, xtx, beta, p)
  beta_new <- t(rmvnorm(1,as.vector(beta) + c^2*as.vector(grad), c*diag(ncolx)))
  logr <- target(y,x,xtx,beta_new,n)-target(y,x,xtx,beta,n)
  if(log(runif(1)) < logr){
    beta <- beta_new
    num_accepted <- num_accepted+1
  }
  accept_ratio <- num_accepted/s
  beta.chain[,s] <- beta
}
```

```{r, echo=F, fig.align="center"}
par(mfrow=c(2,3))
plot(beta.chain[1,],type="l")
plot(beta.chain[2,],type="l")
plot(beta.chain[3,],type="l")
plot(beta.chain[4,],type="l")
plot(beta.chain[5,],type="l")
plot(beta.chain[6,],type="l")
plot(beta.chain[7,],type="l")
df <- rbind(
  "Bayes" = rowMeans(beta.chain),
  "GLM" = coef(glm(y~x[,-1],family=binomial(link="probit")))
)
df
```
These chains look crazy, but I'm not sure what's happening. I think the tuning parameter $c$ is the culprit. 






\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (7).}]
Describe and implement an adaptive Metropolis-Hastings algorithm designed to obtain a MC with stationary distribution $p(\beta|Y)$
\end{tcolorbox}
I think in this in this problem, I'll make proposals at step $s$ such as 
$$
\beta^{*}\sim \beta^{(s)}+N(0,c\Sigma^{(s)})
$$
and adaptively change $c$ based on my acceptance ratio.

```{r, fig.align="center"}
data <- infert
y <- data$case
data$education <- as.numeric(as.factor(data$education))
edu1 <- data$education==2; edu2 <- data$education==3
x <- cbind(rep(1,n),data$age, data$parity, edu1, edu2, data$spontaneous, data$induced)
target <- function(y, x, xtx, beta, n){
  p <- exp(x%*%beta)/(1+exp(x%*%beta))
  return(sum(y*log(p)+(1-y)*log(1-p))-1/(2*n)*t(beta)%*%xtx%*%beta)
}
samples <- 5000
n <- nrow(x); ncolx <- ncol(x)
xtx <- t(x)%*%x
xtx_inv <- solve(xtx)
beta <- t(rmvnorm(1,rep(0,ncol(x)),xtx_inv*n))
beta.chain <- matrix(0,nrow=ncolx,ncol=samples)
beta.chain[,1] <- beta
num_accepted <- accept_ratio <- 0
c <- 10
for(s in 2:samples){
  beta_new <- t(rmvnorm(1,as.vector(beta), c*xtx_inv))
  logr <- target(y,x,xtx,beta_new,n)-target(y,x,xtx,beta,n)
  if(log(runif(1)) < logr){
    beta <- beta_new
    num_accepted <- num_accepted+1
  }
  accept_ratio <- num_accepted/s
  if(accept_ratio > 0.4){
    c <- c+1
  }else{
    c <- c/2
  }
  beta.chain[,s] <- beta
}
```

```{r, echo=F, fig.align="center"}
par(mfrow=c(3,3))
plot(beta.chain[1,],type="l")
plot(beta.chain[2,],type="l")
plot(beta.chain[3,],type="l")
plot(beta.chain[4,],type="l")
plot(beta.chain[5,],type="l")
plot(beta.chain[6,],type="l")
plot(beta.chain[7,],type="l")
df <- rbind(
  "Bayes" = rowMeans(beta.chain),
  "GLM" = coef(glm(y~x[,-1],family=binomial(link="logit")))
)
df
```
