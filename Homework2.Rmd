---
title: "Homework 1"
author: "Ian Frankenburg"
date: "4/7/2020"
header-includes:
   - \usepackage{bm}
   - \usepackage{algorithmic}
   - \usepackage[]{algorithm2e}
   - \usepackage{tikz,lipsum,lmodern}
   - \usepackage[most]{tcolorbox}
   - \usepackage{setspace}
   - \usepackage{cancel}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.pos = 'H')
library(kableExtra)
library(latex2exp)
library(mvtnorm)
library(invgamma)
library(lars)
library(matrixStats)
library(statmod)
library(glmnet)
library(tidyverse)
library(dplyr)
library(tidyr)
library(Rcpp)
```
# Generalized Linear Models
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (1).}]
Describe and implement a Metropolis-Hastings algorithm designed to obtain a MC with stationary distribution $p(\beta|Y)$
\end{tcolorbox}
First I want to think about the likelihood times prior: $\prod p(y_i|X_i,\beta)\times p(\beta|X_i)$. This is theoretically how I'd do it, but a major conceptual point here is that it's going to be hard to generate a sensible multivariate proposal distribution. Instead, I'm gonna sample component-wise ie sample each $\beta_j$ one at a time. The downside of this is mixing rate: Imagine I could sample a sub-vector of $\pmb\beta$. Then I could take advantage of possible correlation to make better proposals and speed up convergence.
This will give me a posterior target, afterwhich I can implement an MCMC-type algirithm
```{r, eval=F}
logtarget <- function(y,X, beta){
  n <- nrow(X)
  likelihood <- p <- 0
  for(i in 1:n){
    p <- pnorm(t(X[i,])%*%beta) 
    likeihood <- log(p)^y[i] + log(1-p)^(1-y[i]) + likeihood
  }
  return(likelihood - 1/(2*n) *t(X%*%beta)%*%(X%*%beta))
}
fit <- summary(glm(y~X,family=binomial(link="probit")))
beta <- fit$coeff[,1]
for(s in 1:samples){
  # propose new beta based on symmetric distribution
  
  # evaluate step in MH
  
}
```

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (2).}]
Describe and implement a data augmented (DA-MCMC) strategy targeting $p(\beta|Y)$
\end{tcolorbox}
The original model is 
$$
P(y_i=1|x_i,\beta)=\Phi(x_i^\top\beta).
$$
This is equivalent to the model $P(y_i=1|x_i,\beta)=P(\pmb 1(z_i>0)=1)=P(z_i>0)$, where $z_i\stackrel{\text{iid}}{\sim} N(x_i^\top\beta,1)$.
This follows immediately since 
$$
\int_{-\infty}^{x_i^\top\beta} N(t;\ 0, 1)dt=\int_{-\infty}^{0}N(z_i;x_i^\top\beta,1)dz_i
$$
by a change of variables $z_i:=t-x_i^\top\beta$. Thus $\Phi(x_i^\top\beta)=P(z_i>0)$.

In defining the latent model, the full conditionals $p(\beta|\pmb y, z)$, $p(\pmb z|\pmb y,\pmb \beta)$ become tractible, so I can use a Gibbs sampler.

I'll start with $p(\pmb\beta|\pmb y, \pmb z)$
$$
\begin{aligned}
p(\pmb\beta|\pmb y, \pmb z)&\propto p(\pmb y, \pmb z|\pmb\beta)p(\pmb\beta)\\
&=\xcancel{p(\pmb y|\pmb\beta, \pmb z)}p(\pmb z|\pmb \beta)p(\pmb\beta)=p(\pmb z|\pmb \beta)p(\pmb\beta)\\
&=N(\pmb z;X\pmb\beta,\pmb I)N(\pmb\beta;\pmb 0,n(X^\top X)^{-1})\\
\end{aligned}
$$

Now for $p(\pmb z|\pmb y, \pmb \beta)$.
$$
\begin{aligned}
p(\pmb z|\pmb y, \pmb \beta)&\propto p(\pmb y, \pmb \beta, \pmb z)\\
&\propto p(\pmb y|\pmb\beta,\pmb z)p(\pmb z|\pmb\beta)\xcancel{p(\pmb\beta)}\\
&\propto\prod_{i=1}^n[\pmb 1(y_i=1)\pmb 1(z_i>0)+\pmb 1(y_i=0)\pmb 1(z_i<0)]N(\pmb z;X\pmb \beta,\pmb I)\\
\end{aligned}
$$
Since our sampling model assumes the $y$'s are independent, so are the $z$'s and I can sample the full conditionals independently, i.e.
$$
\begin{aligned}
p(z_i|\pmb y, \pmb \beta)&\propto p(\pmb y, \pmb \beta, z_i)\propto p(y|\pmb\beta,z_i)p(z_i\pmb|\beta)\\
&\propto1(y_i=1)\pmb 1(z_i>0)+\pmb 1(y_i=0)\pmb 1(z_i<0)]N(z_i;x_i^\top\pmb \beta,1)\\
&=\begin{cases} 
      N(z_i;x_i^\top\pmb \beta,1)*\pmb1_{[0,\infty)}(z_i)  & \text{if }y_i=1\\
      N(z_i;x_i^\top\pmb \beta,1)*\pmb1_{(-\infty,0)}(z_i) & \text{if }y_i=0\\
   \end{cases}
\end{aligned}
$$
Now I can implement a Gibbs sampler to iteratively draw from these conditionals.

\begin{algorithm}
\DontPrintSemicolon
\setstretch{1.5}
  \KwResult{Samples from joint posterior $p(\pmb\beta,\pmb z|\pmb y)$ }
  \For{s in \# samples}{
    
}
\end{algorithm}

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (5).}]
For logit model, describe and implement a data augmented (DA-MCMC) strategy targeting $p(\beta|Y)$
\end{tcolorbox}

Similar to the probit case, the posterior is intractible but can be written as
$$
\begin{aligned}
p(\beta|\pmb y)\propto&\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}p(\beta)\\
=&\prod_{i=1}^n \Big[\frac{e^{x_i^\top\beta}}{1+e^{x_i^\top\beta}}\Big]^{y_i}\Big[1-\frac{e^{x_i^\top\beta}}{1+e^{x_i^\top\beta}}\Big]^{1-y_i}N(\pmb\beta;0,n(X^\top X)^{-1})\\
=&\prod_{i=1}^n\frac{(e^{x_i^\top\beta})^{y_i}}{1+e^{x_i^\top\beta}}N(\pmb\beta;0,n(X^\top X)^{-1})
\end{aligned}
$$
This will be my target in the random walk MH.
Therefore, an algorithm might be 
\begin{algorithm}
\DontPrintSemicolon
\setstretch{1.5}
  \KwResult{Samples from joint posterior $p(\pmb\beta,\pmb z|\pmb y)$ }
  \For{s in \# samples}{
    $\ell(\beta) := \log L(\beta)=\sum_{i=1}^ny_ix_i^\top\beta-\log(1+e^{x_i^\top\beta})-\frac{1}{2n}\beta^\top(X^\top X)\beta$
}
\end{algorithm}

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (6).}]
Describe and implement a Langevin-Hastings algorithm designed to obtain a MC with stationary distribution $p(\beta|Y)$
\end{tcolorbox}
The Langevin-Hastings algorithm will work by utilizing the a 2^{nd} order Taylor approximation of the target distribution. I'm going to use a component-wise MCMC algorithm, so I'm going to make proposals for each $\beta_j$
Therefore, my proposal for $\beta_j$ at iteration $s$ will be something like
$$
\beta_j^*\sim N\Big(\beta_j^{(s)}+\frac{1}{2}\sigma^2\frac{\partial}{\partial \beta_j^{(s)}}\log\big(p(\pmb y|\beta_j)p(\pmb\beta)\big), \sigma^2\pmb I\Big)
$$

Earlier I showed the poserior is proportional to 
$$
\prod_{i=1}^n\frac{(e^{x_i^\top\beta})^{y_i}}{1+e^{x_i^\top\beta}}N(\pmb\beta;0,n(X^\top X)^{-1})
$$
so the log-posterior is 
$$
\begin{aligned}
&\sum_{i=1}^n y_ix_i^\top\pmb\beta-\log(1+e^{x_i^\top\pmb\beta})-\frac{1}{2n}\beta^\top(X^\top X)\beta\\
\Rightarrow&\nabla\Big\{\sum_{i=1}^n y_ix_i^\top\pmb\beta-\log(1+e^{x_i^\top\pmb\beta})-\frac{1}{2n}\pmb \beta^\top(X^\top X)\beta\Big\}\\
=&\sum_{i=1}^n[y_i -\frac{e^{x_i^\top\pmb \beta}}{1+e^{x_i^\top\pmb \beta}}]x_i^\top-\frac{1}{n}\pmb \beta^\top(X^\top X)
\end{aligned}
$$
Therefore, the proposal will be of the form
$$
\begin{aligned}
\beta_j^*&\sim N(\beta_j+\frac{1}{2}\sigma^2\frac{ \partial}{\partial\beta_j}\text{log-target}, \sigma^2)\\
\end{aligned}
$$
Where $\frac{\partial}{\partial\beta_j}\text{log-target}$ is the $j$-th element of the gradient $\nabla$ computed previously.

\newpage
\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!white,title={Part (7).}]
Describe and implement an adaptive Metropolis-Hastings algorithm designed to obtain a MC with stationary distribution $p(\beta|Y)$
\end{tcolorbox}
